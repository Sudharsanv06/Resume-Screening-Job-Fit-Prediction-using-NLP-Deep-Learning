{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6a309a6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import preprocessing module\n",
        "from preprocess import clean_text, clean_resumes\n",
        "\n",
        "# TensorFlow for tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Scikit-learn for label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0f34ebe",
      "metadata": {},
      "source": [
        "## 1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3f89582d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: 30 resumes, 2 columns\n",
            "Job categories: 14\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Resume</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Experienced Python developer with 5+ years in ...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Senior Java developer with expertise in Spring...</td>\n",
              "      <td>Java Developer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Full-stack web developer skilled in React, Nod...</td>\n",
              "      <td>Web Developer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data scientist with expertise in Python, R, an...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DevOps engineer with experience in Docker, Kub...</td>\n",
              "      <td>DevOps Engineer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Resume         Category\n",
              "0  Experienced Python developer with 5+ years in ...   Data Scientist\n",
              "1  Senior Java developer with expertise in Spring...   Java Developer\n",
              "2  Full-stack web developer skilled in React, Nod...    Web Developer\n",
              "3  Data scientist with expertise in Python, R, an...   Data Scientist\n",
              "4  DevOps engineer with experience in Docker, Kub...  DevOps Engineer"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the resume dataset\n",
        "df = pd.read_csv('../dataset/resumes.csv')\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape[0]} resumes, {df.shape[1]} columns\")\n",
        "print(f\"Job categories: {df['Category'].nunique()}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6963b0be",
      "metadata": {},
      "source": [
        "## 2. Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bd11068c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BEFORE CLEANING:\n",
            "================================================================================\n",
            "Experienced Python developer with 5+ years in machine learning and deep learning. Proficient in TensorFlow, PyTorch, scikit-learn, and NLP. Built recommendation systems and predictive models. Strong u\n",
            "\n",
            "================================================================================\n",
            "AFTER CLEANING:\n",
            "================================================================================\n",
            "experienced python developer with years in machine learning and deep learning proficient in tensorflow pytorch scikitlearn and nlp built recommendation systems and predictive models strong understandi\n"
          ]
        }
      ],
      "source": [
        "# Test cleaning on a single resume\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEFORE CLEANING:\")\n",
        "print(\"=\"*80)\n",
        "print(df['Resume'].iloc[0][:200])\n",
        "\n",
        "cleaned_sample = clean_text(df['Resume'].iloc[0])\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AFTER CLEANING:\")\n",
        "print(\"=\"*80)\n",
        "print(cleaned_sample[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "de419751",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”„ Cleaning all resumes...\n",
            "âœ… Text cleaning complete!\n",
            "\n",
            "Sample cleaned resume:\n",
            "experienced python developer with years in machine learning and deep learning proficient in tensorflow pytorch scikitlearn and nlp built recommendatio\n"
          ]
        }
      ],
      "source": [
        "# Clean all resumes\n",
        "print(\"\\nðŸ”„ Cleaning all resumes...\")\n",
        "df['Cleaned_Resume'] = clean_resumes(df['Resume'].tolist(), remove_stopwords=False)\n",
        "\n",
        "print(\"âœ… Text cleaning complete!\")\n",
        "print(f\"\\nSample cleaned resume:\")\n",
        "print(df['Cleaned_Resume'].iloc[0][:150])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5a6e44",
      "metadata": {},
      "source": [
        "## 3. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6af1621a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Tokenizer fitted!\n",
            "Vocabulary size: 286\n",
            "\n",
            "Most common words:\n",
            "[('<OOV>', 1), ('and', 2), ('with', 3), ('in', 4), ('experience', 5), ('proficient', 6), ('developer', 7), ('strong', 8), ('of', 9), ('expertise', 10), ('data', 11), ('knowledge', 12), ('python', 13), ('learning', 14), ('built', 15), ('react', 16), ('building', 17), ('web', 18), ('engineer', 19), ('testing', 20)]\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer\n",
        "# num_words limits vocabulary to most common words (helps reduce model complexity)\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "\n",
        "# Fit tokenizer on cleaned text\n",
        "tokenizer.fit_on_texts(df['Cleaned_Resume'])\n",
        "\n",
        "# Get vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
        "\n",
        "print(f\"\\nâœ… Tokenizer fitted!\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"\\nMost common words:\")\n",
        "print(list(tokenizer.word_index.items())[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "66c15c8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Text converted to sequences!\n",
            "\n",
            "Sample sequence (first 20 tokens):\n",
            "[109, 13, 7, 3, 21, 4, 22, 14, 2, 37, 14, 6, 4, 38, 110, 62, 2, 63, 15, 111]\n",
            "\n",
            "Original text: experienced python developer with years in machine learning and deep learning proficient in tensorfl\n"
          ]
        }
      ],
      "source": [
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['Cleaned_Resume'])\n",
        "\n",
        "print(f\"\\nâœ… Text converted to sequences!\")\n",
        "print(f\"\\nSample sequence (first 20 tokens):\")\n",
        "print(sequences[0][:20])\n",
        "print(f\"\\nOriginal text: {df['Cleaned_Resume'].iloc[0][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daebb024",
      "metadata": {},
      "source": [
        "## 4. Sequence Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3325bcba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length statistics:\n",
            "Min length: 22\n",
            "Max length: 33\n",
            "Mean length: 25.30\n",
            "Median length: 24.50\n"
          ]
        }
      ],
      "source": [
        "# Calculate sequence lengths\n",
        "sequence_lengths = [len(seq) for seq in sequences]\n",
        "\n",
        "print(\"Sequence length statistics:\")\n",
        "print(f\"Min length: {min(sequence_lengths)}\")\n",
        "print(f\"Max length: {max(sequence_lengths)}\")\n",
        "print(f\"Mean length: {np.mean(sequence_lengths):.2f}\")\n",
        "print(f\"Median length: {np.median(sequence_lengths):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3c08c002",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Sequences padded!\n",
            "Max sequence length: 100\n",
            "Padded sequences shape: (30, 100)\n",
            "\n",
            "Sample padded sequence:\n",
            "[109  13   7   3  21   4  22  14   2  37  14   6   4  38 110  62   2  63\n",
            "  15 111  64   2  65  39   8  40   9 112 113 114 115   2 116   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "# Set max sequence length (use median or a reasonable value)\n",
        "max_length = 100  # You can adjust based on your data\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "print(f\"\\nâœ… Sequences padded!\")\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "print(f\"Padded sequences shape: {padded_sequences.shape}\")\n",
        "print(f\"\\nSample padded sequence:\")\n",
        "print(padded_sequences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aefa6d6",
      "metadata": {},
      "source": [
        "## 5. Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "52987945",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Labels encoded!\n",
            "Number of classes: 14\n",
            "\n",
            "Class mapping:\n",
            "0: Backend Developer\n",
            "1: Business Analyst\n",
            "2: Cloud Architect\n",
            "3: Data Analyst\n",
            "4: Data Engineer\n",
            "5: Data Scientist\n",
            "6: DevOps Engineer\n",
            "7: Frontend Developer\n",
            "8: Java Developer\n",
            "9: Mobile Developer\n",
            "10: Python Developer\n",
            "11: QA Engineer\n",
            "12: Security Analyst\n",
            "13: Web Developer\n"
          ]
        }
      ],
      "source": [
        "# Initialize label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode categories\n",
        "encoded_labels = label_encoder.fit_transform(df['Category'])\n",
        "\n",
        "print(f\"\\nâœ… Labels encoded!\")\n",
        "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
        "print(f\"\\nClass mapping:\")\n",
        "for i, category in enumerate(label_encoder.classes_):\n",
        "    print(f\"{i}: {category}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7001ae72",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample data after preprocessing:\n",
            "Original text: Experienced Python developer with 5+ years in machine learning and deep learning. Proficient in Tens...\n",
            "Cleaned text: experienced python developer with years in machine learning and deep learning proficient in tensorfl...\n",
            "Sequence: [109  13   7   3  21   4  22  14   2  37  14   6   4  38 110  62   2  63\n",
            "  15 111]...\n",
            "Category: Data Scientist\n",
            "Encoded label: 5\n"
          ]
        }
      ],
      "source": [
        "# Display sample data\n",
        "print(\"\\nSample data after preprocessing:\")\n",
        "print(f\"Original text: {df['Resume'].iloc[0][:100]}...\")\n",
        "print(f\"Cleaned text: {df['Cleaned_Resume'].iloc[0][:100]}...\")\n",
        "print(f\"Sequence: {padded_sequences[0][:20]}...\")\n",
        "print(f\"Category: {df['Category'].iloc[0]}\")\n",
        "print(f\"Encoded label: {encoded_labels[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae0fd09",
      "metadata": {},
      "source": [
        "## 6. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f31f8b2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Data split complete!\n",
            "Training samples: 24\n",
            "Testing samples: 6\n",
            "\n",
            "Training set shape: (24, 100)\n",
            "Testing set shape: (6, 100)\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and testing sets (without stratify due to small sample size)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, \n",
        "    encoded_labels, \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Data split complete!\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02bfcbb8",
      "metadata": {},
      "source": [
        "## 7. Save Preprocessing Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bca8ff48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Tokenizer saved to models/tokenizer.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save tokenizer\n",
        "import pickle\n",
        "\n",
        "with open('../models/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"âœ… Tokenizer saved to models/tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b5d3945f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Label encoder saved to models/label_encoder.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save label encoder\n",
        "with open('../models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"âœ… Label encoder saved to models/label_encoder.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "78b278c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Preprocessed data saved:\n",
            "   - models/X_train.npy\n",
            "   - models/X_test.npy\n",
            "   - models/y_train.npy\n",
            "   - models/y_test.npy\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessed data\n",
        "np.save('../models/X_train.npy', X_train)\n",
        "np.save('../models/X_test.npy', X_test)\n",
        "np.save('../models/y_train.npy', y_train)\n",
        "np.save('../models/y_test.npy', y_test)\n",
        "\n",
        "print(\"âœ… Preprocessed data saved:\")\n",
        "print(\"   - models/X_train.npy\")\n",
        "print(\"   - models/X_test.npy\")\n",
        "print(\"   - models/y_train.npy\")\n",
        "print(\"   - models/y_test.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1d26fca1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Preprocessing info saved to results/preprocessing_info.txt\n",
            "\n",
            "PREPROCESSING INFORMATION - DAY 2\n",
            "=====================================\n",
            "\n",
            "Dataset:\n",
            "- Total samples: 30\n",
            "- Training samples: 24\n",
            "- Testing samples: 6\n",
            "\n",
            "Vocabulary:\n",
            "- Vocabulary size: 286\n",
            "- Max words in tokenizer: 5000\n",
            "- OOV token: <OOV>\n",
            "\n",
            "Sequences:\n",
            "- Max sequence length: 100\n",
            "- Padding: post\n",
            "- Truncating: post\n",
            "\n",
            "Labels:\n",
            "- Number of classes: 14\n",
            "- Classes: Backend Developer, Business Analyst, Cloud Architect, Data Analyst, Data Engineer, Data Scientist, DevOps Engineer, Frontend Developer, Java Developer, Mobile Developer, Python Developer, QA Engineer, Security Analyst, Web Developer\n",
            "\n",
            "Text Cleaning:\n",
            "- Lowercasing: Yes\n",
            "- Special characters removed: Yes\n",
            "- Stopwords removed: No\n",
            "- Extra spaces removed: Yes\n",
            "\n",
            "Files Saved:\n",
            "- models/tokenizer.pkl\n",
            "- models/label_encoder.pkl\n",
            "- models/X_train.npy\n",
            "- models/X_test.npy\n",
            "- models/y_train.npy\n",
            "- models/y_test.npy\n",
            "\n",
            "Ready for Day 3: Model Building!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessing info to text file\n",
        "preprocessing_info = f\"\"\"PREPROCESSING INFORMATION - DAY 2\n",
        "=====================================\n",
        "\n",
        "Dataset:\n",
        "- Total samples: {len(df)}\n",
        "- Training samples: {len(X_train)}\n",
        "- Testing samples: {len(X_test)}\n",
        "\n",
        "Vocabulary:\n",
        "- Vocabulary size: {vocab_size}\n",
        "- Max words in tokenizer: 5000\n",
        "- OOV token: <OOV>\n",
        "\n",
        "Sequences:\n",
        "- Max sequence length: {max_length}\n",
        "- Padding: post\n",
        "- Truncating: post\n",
        "\n",
        "Labels:\n",
        "- Number of classes: {len(label_encoder.classes_)}\n",
        "- Classes: {', '.join(label_encoder.classes_)}\n",
        "\n",
        "Text Cleaning:\n",
        "- Lowercasing: Yes\n",
        "- Special characters removed: Yes\n",
        "- Stopwords removed: No\n",
        "- Extra spaces removed: Yes\n",
        "\n",
        "Files Saved:\n",
        "- models/tokenizer.pkl\n",
        "- models/label_encoder.pkl\n",
        "- models/X_train.npy\n",
        "- models/X_test.npy\n",
        "- models/y_train.npy\n",
        "- models/y_test.npy\n",
        "\n",
        "Ready for Day 3: Model Building!\n",
        "\"\"\"\n",
        "\n",
        "with open('../results/preprocessing_info.txt', 'w') as f:\n",
        "    f.write(preprocessing_info)\n",
        "\n",
        "print(\"âœ… Preprocessing info saved to results/preprocessing_info.txt\")\n",
        "print(\"\\n\" + preprocessing_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec8c4a16",
      "metadata": {},
      "source": [
        "## 8. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "aabc989c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DAY 2 SUMMARY - TEXT PREPROCESSING & TOKENIZATION\n",
            "================================================================================\n",
            "\n",
            "âœ… Text Cleaning: Complete\n",
            "   - Lowercasing applied\n",
            "   - Special characters removed\n",
            "   - Extra spaces normalized\n",
            "\n",
            "âœ… Tokenization: Complete\n",
            "   - Vocabulary size: 286\n",
            "   - Max sequence length: 100\n",
            "\n",
            "âœ… Label Encoding: Complete\n",
            "   - Number of classes: 14\n",
            "\n",
            "âœ… Data Split: Complete\n",
            "   - Training: 24 samples\n",
            "   - Testing: 6 samples\n",
            "\n",
            "âœ… All preprocessing artifacts saved!\n",
            "\n",
            "ðŸš€ Ready for Day 3: LSTM Model Building!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Print final summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DAY 2 SUMMARY - TEXT PREPROCESSING & TOKENIZATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nâœ… Text Cleaning: Complete\")\n",
        "print(f\"   - Lowercasing applied\")\n",
        "print(f\"   - Special characters removed\")\n",
        "print(f\"   - Extra spaces normalized\")\n",
        "\n",
        "print(f\"\\nâœ… Tokenization: Complete\")\n",
        "print(f\"   - Vocabulary size: {vocab_size}\")\n",
        "print(f\"   - Max sequence length: {max_length}\")\n",
        "\n",
        "print(f\"\\nâœ… Label Encoding: Complete\")\n",
        "print(f\"   - Number of classes: {len(label_encoder.classes_)}\")\n",
        "\n",
        "print(f\"\\nâœ… Data Split: Complete\")\n",
        "print(f\"   - Training: {len(X_train)} samples\")\n",
        "print(f\"   - Testing: {len(X_test)} samples\")\n",
        "\n",
        "print(f\"\\nâœ… All preprocessing artifacts saved!\")\n",
        "print(f\"\\nðŸš€ Ready for Day 3: LSTM Model Building!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
